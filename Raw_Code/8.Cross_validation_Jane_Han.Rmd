---
title: "Final Project"
author: Zheyu Han
date: "8/14/2021"
output: html_document
---
### Set up
```{r}
suppressMessages(library(TSA))
suppressMessages(library(ggplot2))
suppressMessages(library(urca))
suppressMessages(library(forecast))
# suppressMessages(library(reshape))
suppressMessages(library(tseries))
```

### 1. Load Data and select nationality = "USA"
1). Load dataset
```{r}
# dataPath1 <- "C:/Users/jiang/Desktop/Dataset/Time_Series/Final_Project/thaitourism2"
thai <- read.csv("thaitourism2.csv")
head(thai)
```
```{r}
# No missing value
sum(is.na(thai))
```

2). Subset the data of USA
```{r}
thai_sub <- thai[c('year', 'month', 'nationality', 'tourists')]
usa_all <- subset(thai_sub, nationality == 'USA')
head(usa_all)
```

### 2. Split train and test set

1). Split train and test set
```{r}
usa_raw <- sapply(usa_all['tourists'], as.numeric)
usa <- ts(usa_raw, start = c(2010,1), frequency = 12)

train <- ts(usa[1:72], start = c(2010,1), frequency = 12)
test <- ts(usa[73:84], start = c(2016,1), frequency = 12)
```

2). Plot the training data
```{r}
tsdisplay(train, main = "Number of the American tourists visiting thailand", xlab = "Year", ylab = "Number of tourists")
```

3). Decompose the training set
```{r}
fit_mult <- decompose(train, type = 'multiplicative')
plot(fit_mult)
```

```{r}
fit_mult <- decompose(train, type = 'additive')
plot(fit_mult)
```

## Holt-Winter
### 3. Fit Holt-Winter exponential smoothing method
1). Holt-Winter method - Multiplicative

a. Fit the hw multiplicative with dampen
```{r}
fit_hw <- hw(train, h=12, seasonal = 'mult')
summary(fit_hw)
```

b. Plot the forecast
```{r}
autoplot(usa) + 
  autolayer(fit_hw$mean, series = "forecasted value") +
  ggtitle("Forecasts for monthly number of tourists in Thailand - Holt-Winter Multiplicative") +
  xlab("Year") + ylab("Number of tourists")
```

c. Check Residuals
```{r}
checkresiduals(fit_hw)
```

Basd on the result of Ljung-Box test, the null hypothesis should be rejected. This mean there is still autocorrelation in the residuals.

d. Check accuracy
```{r}
accuracy(fit_hw$mean, test)
```

2). Holt-Winter - Additive

a. Fit the hw additive with dampen 
```{r}
fit_hw2 <- hw(train, h=12, seasonal = 'additive')
summary(fit_hw2)
```

b. Plot the forecast
```{r}
autoplot(usa) + 
  autolayer(fit_hw2$mean, series = "forecasted value") +
  ggtitle("Forecasts for monthly number of tourists in Thailand - Holt-Winter Additive") +
  xlab("Year") + ylab("Number of tourists")
```
```{r}
fit_hw2
```

c. Check residuals
```{r}
checkresiduals(fit_hw2)
```

Reject null hypothesis -> The residuals are still not independently distributed

d. Check accuracy
```{r}
accuracy(fit_hw2$mean, test)
```

## SARIMA
### Box-Cox Transformation
```{r}
lda <- BoxCox.lambda(train)
train_new <- BoxCox(train, lda)
tsdisplay(train_new)
```

### Detrend and Deseasonality
1). Is the transformed data stationary?
```{r}
kpss.test(train_new)
```

The p-value of KPSS test is smaller than the significant level of 0.05. We have to reject the null hypothesis and thus conclude that the data is not stationary. Certain detrend and deseasonality method should be applied

2). Apply seasonal differencing
```{r}
train_seadiff <- diff(train_new, lag = 12)
tsdisplay(train_seadiff)
```

3). Apply nonseasonal differencing
```{r}
train_sea_diff <- diff(train_seadiff, lag = 1)
tsdisplay(train_sea_diff)
```

4). Check stationarity again
```{r}
kpss.test(train_sea_diff)
```

Now the data become stationary.

### Build ARIMA(p,d,q)(P,D,Q)[s]
1). Model 1: Let the auto.arima() function determine the best order of non-seasonal and seasonal differencing
```{r}
arima1 <- auto.arima(train, trace = TRUE, lambda = lda)
arima1
```

2). Set the order of seasonal differencing d to 1 and D to 1
```{r}
arima2 <- auto.arima(train, d = 1, D = 1, trace = TRUE, lambda = lda)
arima2
```

3). Report the resulting p, d, q, P, D, Q, s and the coefficients values for all cases and compare their AICc and BIC values
```{r}
coeff1 <- data.frame(arima1$coef)
coeff1$model <- "ARIMA(1,0,0)(2,1,0)[12] with drift"

coeff2 <- data.frame(arima2$coef)
coeff2$model <- "ARIMA(0,1,1)(0,1,1)[12]"

coeff1
coeff2
```

```{r}
models <- data.frame(arima1[c("aicc", "bic")])
models <- rbind(models, data.frame(arima2[c("aicc", "bic")]))
models$model <- c("ARIMA(1,0,0)(2,1,0)[12] with drift", "ARIMA(0,1,1)(0,1,1)[12]")
models
```

### Plot the residuals
1). Check the residual of ARIMA(1,0,0)(2,1,0)[12] with drift
```{r}
checkresiduals(arima1)
```

2). Check the residual of ARIMA(0,1,1)(0,1,1)[12]
```{r}
checkresiduals(arima2)
```

### Plot forecast
1). ARIMA(1,0,0)(2,1,0)[12] with drift
```{r}
forecast1 <- forecast(arima1, h=12)
forecast1_ts <- ts(forecast1$mean, start = c(2016, 1), frequency = 12)
```


```{r}
autoplot(usa) + 
  autolayer(forecast1_ts, series = "forecasted value") +
  ggtitle("Forecasts for monthly number of tourists in Thailand - ARIMA(1,0,0)(2,1,0)[12] with drift") +
  xlab("Year") + ylab("Number of tourists")
```

2). ARIMA(0,1,1)(0,1,1)[12]
```{r}
forecast2 <- forecast(arima2, h=12)
forecast2_ts <- ts(forecast2$mean, start = c(2016, 1), frequency = 12)
```

```{r}
autoplot(usa) + 
  autolayer(forecast2_ts, series = "forecasted value") +
  ggtitle("Forecasts for monthly number of tourists in Thailand - ARIMA(0,1,1)(0,1,1)[12]") +
  xlab("Year") + ylab("Number of tourists")
```


### Check accuracy
```{r}
acc_arima1 <- accuracy(forecast1_ts, test)
acc_arima1
```

```{r}
acc_arima2 <- accuracy(forecast2_ts, test)
acc_arima2
```
# 5) baseline models
```{r}
h <- 12
Model_Mean <- meanf(train, h) 
Model_Naive <- naive(train, h) 
Model_SNaive <- snaive(train, h)
Model_Drift <- rwf(train, h, drift=TRUE)
autoplot(usa) +
  autolayer(Model_Mean$mean, series="Mean") +
  autolayer(Model_Naive$mean, series="Naïve") +
  autolayer(Model_SNaive$mean, series="Seasonal naïve") +
  autolayer(Model_Drift$mean, series="Drift") +
  ggtitle("Forecasts for monthly Thailand visitors") + ylab("Visitors")
```

```{r}
autoplot(usa) +
  autolayer(fit_hw$mean, series = "HW-Multiplicative") +
  autolayer(fit_hw2$mean, series= "HW-Additive") +
  autolayer(forecast1_ts, series="SARIMA-(1,0,0)(2,1,0)") +
  autolayer(forecast2_ts, series="SARIMA-(0,1,1)(0,1,1)") +
  ggtitle("Forecasts for monthly Thailand visitors") + ylab("Visitors")

``` 

```{r}
autoplot(usa) +
  autolayer(forecast1_ts, series="SARIMA-(1,0,0)(2,1,0)") +
  autolayer(forecast2_ts, series="SARIMA-(0,1,1)(0,1,1)") +
  ggtitle("Forecasts for monthly Thailand visitors") + ylab("Visitors")

```
## Cross validation

```{r}
ets_model <- ets(train)
ets_model 
```

### Define Global Variables
```{r}
# Global Variables
k <- 40 # minimum number of samples required to train the model
n <- length(train) # Number of data points
H <- 12 # Forecast horizon
p <- 12 # Period
st <- tsp(usa)[1]+(k-2)/p #  gives the start time in time units,

```

### Define model functions to three models with train_set as the input 
### Then we can use the function variable to cross-validation part
```{r}
sarima_model1 <- function(train_set) {
  Arima(train_set, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=p),lambda = lda, method="ML")
}
sarima_model2 <- function(train_set) {
  Arima(train_set, order=c(1,0,0), seasonal=list(order=c(2,1,0), period=p), include.drift = TRUE, lambda = lda, method="ML")
}
ets_model <- function(train_set) {
  ets(train_set, model = 'ANA', lambda = lda)
}

```

### Define cross validaion function with model as the input
```{r}
cv_models <- function(model) {
  model_expand_mae <- matrix(NA,n-k,H)
  model_slide_mae <- matrix(NA,n-k,H)
  model_expand_rmse <- matrix(NA,n-k,H)
  model_slide_rmse <- matrix(NA,n-k,H)
  model_expand_aic <- matrix(NA,n-k)
  model_slide_aic <- matrix(NA,n-k)
    
  for(i in 1:(n-k))
  {
    # Expanding Window 
    train_1 <- window(train, end=st + i/p)  ## Window Length: k+i
    # Sliding Window - keep the training window of fixed length. 
    train_2 <- window(train, start=st+(i-k+1)/p, end=st+i/p) ## Window Length: k
    test <- window(train, start=st + (i+1)/p, end=st + (i+H)/p) ## Window Length: H

    fit_1 <- model(train_1)
    fcast_1 <- forecast(fit_1, h=H)
    fit_2 <- model(train_2)
    fcast_2 <- forecast(fit_2, h=H)
    model_expand_mae[i,1:length(test)] <- abs(fcast_1[['mean']]-test)
    model_slide_mae[i,1:length(test)] <- abs(fcast_2[['mean']]-test)
    model_expand_rmse[i,1:length(test)] <- sqrt(mean((fcast_1[['mean']]-test)^2))
    model_slide_rmse[i,1:length(test)] <- sqrt(mean((fcast_2[['mean']]-test)^2))
    model_expand_aic[i] <- fit_1$aicc
    model_slide_aic[i] <- fit_2$aicc
    model_list <- list(model_expand_mae, model_slide_mae, model_expand_rmse, model_slide_rmse, model_expand_aic, model_slide_aic)
  }
  return(model_list)
}

```

```{r}
arima_no_drift <- cv_models(sarima_model1)
arima_with_drift <- cv_models(sarima_model2)
ets_results <- cv_models(ets_model)
```
#### 1) Mean Absolute Forecast Error (MAE) vs forecast horizon.

```{r}
# par(xpd=T, mar=par()$mar+c(0,0,0,8))
plot(1:12, colMeans(arima_no_drift[[1]],na.rm=TRUE), type="l",col=1,xlab="horizon", ylab="MAE", main = "Mean Absolute Forecast Error (MAE) vs forecast horizon", ylim=c(3000, 15000))
lines(1:12, colMeans(arima_no_drift[[2]],na.rm=TRUE), type="l",col=2)
# so turn off clipping:
par(xpd=TRUE)
legend("topleft",legend=c("ARIMA(0,1,1)(0,1,1)[12] - Expanding Window","ARIMA(0,1,1)(0,1,1)[12] - Sliding window"),col=1:2,lty=1, pch = 1, cex = 0.75)
# nset=c(-0.3,0), 
```

#### 2) Root-square Forecast Error (RMSE) vs forecast horizon.

```{r}
plot(1:12, colMeans(arima_no_drift[[3]],na.rm=TRUE), type="l",col=1,xlab="horizon", ylab="RMSE", main = "Root-square Forecast Error (RMSE) vs forecast horizon", ylim=c(7500, 12000))
lines(1:12, colMeans(arima_no_drift[[4]],na.rm=TRUE), type="l",col=2)

legend("topleft",legend=c("ARIMA(0,1,1)(0,1,1)[12] - Expanding Window","ARIMA(0,1,1)(0,1,1)[12] - Sliding Window"),col=1:2,lty=1, pch = 1, cex = 0.75)

```
#### 3) AICc vs iteration number
```{r}
plot(1:32, arima_no_drift[[5]], type="l",col=1,xlab="iterations", ylab="AICc", main = "AICc vs iteration number", ylim=c(200, 1100) )
lines(1:32, arima_no_drift[[6]], type="l",col=2)
lines(1:32, arima_with_drift[[5]], type="l",col=3)
lines(1:32, arima_with_drift[[6]], type="l",col=4)
lines(1:32, ets_results[[5]], type="l",col=5)
lines(1:32, ets_results[[6]], type="l",col=6)

legend("topleft",legend=c("ARIMA no drift - Expanding Window","ARIMA no drift - Sliding Window", 'ARIMA with drift - Expanding Window', 'ARIMA with drift -Sliding Window', "ETS - Expanding Window","ETS - Sliding Window"),col=1:6,lty=1, pch = 1, cex = 0.75)
```

```{r}
fit_sarima_wdrift <- sarima_model2(train)
fcast_sarima_wdrift <- forecast(fit_sarima_wdrift, h=H)
rmse(fcast_sarima_wdrift$mean, test)
```
```{r}
fit_ets <- ets_model(train)
fcast_ets <- forecast(fit_ets, h=H)
rmse(fcast_ets$mean, test)
```

## SARIMA model with regression 

```{r}
library(openxlsx)
support_data <- read.xlsx('currency_and_temperature_data.xlsx')
head(support_data)
```
```{r}
colnames(support_data) <- c('year','month','currency','temperature')
head(support_data)
```
```{r}
usa <- ts(usa_raw, start = c(2010,1), frequency = 12)
train <- ts(usa[1:72], start = c(2010,1), frequency = 12)
test <- ts(usa[73:84], start = c(2016,1), frequency = 12)

p_df <- c(usa_raw,support_data$currency,support_data$temperature)
head(p_df)
```
```{r}
df <- merge(usa_all, support_data, by=c('year', 'month'), all = TRUE)
df <- df[order(df[,1], df[,2] ),][, c('tourists', 'currency', 'temperature')]
df_train <- df[1:72, ]
df_test <- df[73:84, ]
```

```{r}
whole <- ts(support_data[, c('currency', 'temperature')], start = c(2010,1), frequency = 12)
currency_train<- ts(support_data$currency[1:72], start = c(2010,1), frequency = 12)
currency_test <- ts(support_data$currency[73:84], start = c(2016,1), frequency = 12)
whole_train <- ts(support_data$temperature[1:72], start = c(2010,1), frequency = 12)
temperature_test <- ts(support_data$temperature[73:84], start = c(2016,1), frequency = 12)
```

```{r}
tslm_motel <- tslm(train ~ currency_train + temperature_train)
forecast(tslm_motel, h=12)
```

```{r}
fit <- tslm(train ~ trend + season)
plot(forecast(fit, h=20))
```

```{r}
arima_with_reg <- auto.arima(train, xreg = support_train, seasonal = TRUE, lambda = lda) 
summary(arima_with_reg)
```
```{r}
checkresiduals(arima_with_reg)
```

```{r}
arima_with_reg_no_lambda <- auto.arima(train, xreg = support_train, seasonal = TRUE) 
summary(arima_with_reg_no_lambda)
```
```{r}
checkresiduals(arima_with_reg_no_lambda)

```


```{r}
support_naive_model <- naive(support_train, h = 12) 
autoplot(support_train) + 
  autolayer(support_naive_model, series = 'Naive mode') +
  ggtitle("Forecasts for monthly CPI with naive model") +
  ylab("CPI")
```


```{r}
forecast_arima_with_reg <- forecast(arima_with_reg, xreg = support_naive_model$mean, h = 12)
rmse(forecast_arima_with_reg$mean, test)
mape(forecast_arima_with_reg$mean, test)
```


```{r}
fcst_arima_with_reg_no_lambda <- forecast(arima_with_reg_no_lambda, xreg = support_naive_model$mean, h = 12) 
rmse(fcst_arima_with_reg_no_lambda$mean, test)
mape(fcst_arima_with_reg_no_lambda$mean, test)
```





